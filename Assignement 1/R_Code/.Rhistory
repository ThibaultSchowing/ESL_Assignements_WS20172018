par(mfrow=c(1,2))
plot(boost.hitters, i="CAtBat")
plot(boost.hitters, i="CWalks")
# Incrementing shrinkage parameter from 0 to 1 with 0.01 increment
#Table containing Delta and MSE
test_mse = array(dim = 100)
train_mse = array(dim = 100)
deltas = array(dim = 100)
i = 1
for(shrkg in seq(0, 1, 0.01)) {
boost.hitters = gbm(Salary~.,
data=trainset,
distribution = "gaussian",
n.trees=1000,
interaction.depth = 4,
shrinkage=shrkg)
salary.boost=predict(boost.hitters, newdata=testset, n.trees=1000)
boost.hitters$train.error
# Print MSE and delta
MSE = mean((salary.boost - testset$Salary)^2)
T_MSE = mean(boost.hitters$train.error)
print(paste(shrkg, "   ", MSE))
test_mse[i] <- MSE
train_mse[i] <- T_MSE
deltas[i] <- shrkg
i = i+1
}
#Get the delta giving the lowest MSE
best_shrinkage = deltas[match(min(test_mse), test_mse)]
boosting_MSE = min(test_mse)
# HERE TODO: Plot test_mse, train_mse and Deltas(x axis)
plot(deltas, test_mse,xlab="Shrinkage Parameter", col="red", ylim=c(0,1.5), ylab="Train MSE (Black) - Test MSE (Red)")
par(new=TRUE)
plot(deltas, train_mse,xlab="Shrinkage Parameter", ylim=c(0,1.5), ylab="")
# Around 0.2 the train error is low and the test error hasn't gone up too fast.
# C
# Linear regression
lm.fit = lm(Salary~., data=trainset)
summary(lm.fit)
lm.pred = predict(lm.fit,testset)
least_square_MSE = mean((testset$Salary - lm.pred)^2)
# Ridge regression
x = model.matrix(Salary~.,completedata)[,-1]
y = completedata$Salary
train=sample(1:nrow(x), nrow(x)/2)
test=(-train )
y.test = y[test]
library(glmnet)
grid=10^seq(10,-2,length=100)
ridge.mod=glmnet(x[train,],y[train], alpha=0, lambda=grid)
ridge.pred = predict(ridge.mod, s=4, newx = x[test,])
ridge_MSE = mean(((ridge.pred - y.test)^2))
print(boosting_MSE)
print(least_square_MSE)
print(ridge_MSE)
# Boosting MSE is a lot lower than linear regression or ridge (1/2)
# D
# Boost variables
summary(boost.hitters)
# Here most important variable is CAtBat
# For Ridge regression:
cv.out=cv.glmnet(x[train,],y[train],alpha=1)
plot(cv.out)
bestlam=cv.out$lambda.min
cv.out$lambda
# For linear regression it is LeagueN. In second it is division (with absolute value)
sort(abs(lm.fit$coefficients))
# E
plot(deltas, test_mse,xlab="Shrinkage Parameter", col="red", ylim=c(0,1.5), ylab="Train MSE (Black) - Test MSE (Red)")
par(new=TRUE)
fitted(lm(deltas~test_mse + I(test_mse^2)))
plot(deltas, train_mse,xlab="Shrinkage Parameter", ylim=c(0,1.5), ylab="")
plot(deltas, test_mse,xlab="Shrinkage Parameter", col="red", ylim=c(0,1.5), ylab="Train MSE (Black) - Test MSE (Red)", fitted(lm(deltas~test_mse + I(test_mse^2))))
abline(lm(deltas~test_mse + I(test_mse^2)))
plot(deltas, test_mse,xlab="Shrinkage Parameter", col="red", ylim=c(0,1.5), ylab="Train MSE (Black) - Test MSE (Red)")
par(new=TRUE)
plot(fitted(lm(deltas~test_mse + I(test_mse^2))))
par(new=TRUE)
plot(deltas, train_mse,xlab="Shrinkage Parameter", ylim=c(0,1.5), ylab=""))
lm.fit = lm(Salary~., data=trainset)
plot(deltas, test_mse,xlab="Shrinkage Parameter", col="red", ylim=c(0,1.5), ylab="Train MSE (Black) - Test MSE (Red)")
par(new=TRUE)
plot(fitted(lm(deltas~test_mse + I(test_mse^2))))
par(new=TRUE)
plot(deltas, train_mse,xlab="Shrinkage Parameter", ylim=c(0,1.5), ylab=""))
par(new=TRUE)
plot(fitted(lm(deltas~test_mse + I(test_mse^2))),type="l")
plot(deltas, test_mse,xlab="Shrinkage Parameter", col="red", ylim=c(0,1.5), ylab="Train MSE (Black) - Test MSE (Red)")
par(new=TRUE)
plot(fitted(lm(deltas~test_mse + I(test_mse^2))), ylim=c(0,1.5),type="l")
par(new=TRUE)
plot(deltas, train_mse,xlab="Shrinkage Parameter", ylim=c(0,1.5), ylab=""))
plot(deltas, train_mse,xlab="Shrinkage Parameter", ylim=c(0,1.5), ylab="")
plot(deltas, test_mse,xlab="Shrinkage Parameter", col="red", ylim=c(0,1.5), ylab="Train MSE (Black) - Test MSE (Red)")
par(new=TRUE)
plot(lines(predict(lm(deltas~test_mse + I(test_mse^2))), ylim=c(0,1.5),type="l"))
par(new=TRUE)
plot(deltas, train_mse,xlab="Shrinkage Parameter", ylim=c(0,1.5), ylab="")
plot(lines(predict(lm(deltas~test_mse + I(test_mse^2))), ylim=c(0,1.5)))
plot(deltas, test_mse,xlab="Shrinkage Parameter", col="red", ylim=c(0,1.5), ylab="Train MSE (Black) - Test MSE (Red)")
par(new=TRUE)
plot(lines(predict(lm(deltas~test_mse + I(test_mse^2))), ylim=c(0,1.5)))
plot(deltas, test_mse,xlab="Shrinkage Parameter", col="red", ylim=c(0,1.5), ylab="Train MSE (Black) - Test MSE (Red)")
par(new=TRUE)
lines(predict(lm(deltas~test_mse + I(test_mse^2))), ylim=c(0,1.5))
par(new=TRUE)
plot(deltas, train_mse,xlab="Shrinkage Parameter", ylim=c(0,1.5), ylab="")
lm.fit = lm(Salary~., data=trainset)
plot(deltas, test_mse,xlab="Shrinkage Parameter", col="red", ylim=c(0,1.5), ylab="Train MSE (Black) - Test MSE (Red)")
par(new=TRUE)
lines(predict(lm(test_mse~deltas + I(deltas^2))), ylim=c(0,1.5))
par(new=TRUE)
plot(deltas, train_mse,xlab="Shrinkage Parameter", ylim=c(0,1.5), ylab="")
lm.fit = lm(Salary~., data=trainset)
plot(deltas, test_mse,xlab="Shrinkage Parameter", col="red", ylim=c(0,1.5), ylab="Train MSE (Black) - Test MSE (Red)")
par(new=TRUE)
lines(predict(test_mse), col='red', lwd=2)
par(new=TRUE)
plot(deltas, train_mse,xlab="Shrinkage Parameter", ylim=c(0,1.5), ylab="")
plot(deltas, test_mse,xlab="Shrinkage Parameter", col="red", ylim=c(0,1.5), ylab="Train MSE (Black) - Test MSE (Red)")
par(new=TRUE)
lo <- loess(delta, test_mse)
lo <- loess(deltas, test_mse)
axx = seq(0, 1, 0.01)
lo <- loess(test_mse, deltas)
test_mse
deltas
lo <- loess(test_mse~deltas)
lines(predict(lo), col='red', lwd=2)
par(new=TRUE)
plot(deltas, train_mse,xlab="Shrinkage Parameter", ylim=c(0,1.5), ylab="")
plot(deltas, test_mse,xlab="Shrinkage Parameter", col="red", ylim=c(0,1.5), ylab="Train MSE (Black) - Test MSE (Red)")
par(new=TRUE)
smoothingSpline = smooth.spline(deltas, test_mse, spar=0.35)
lines(smoothingSpline)
scatter.smooth(deltas,test_mse)
par(new=TRUE)
plot(deltas, train_mse,xlab="Shrinkage Parameter", ylim=c(0,1.5), ylab="")
plot(deltas, test_mse,xlab="Shrinkage Parameter", col="red", ylim=c(0,1.5), ylab="Train MSE (Black) - Test MSE (Red)")
par(new=TRUE)
scatter.smooth(deltas,test_mse)
plot(deltas, test_mse,xlab="Shrinkage Parameter", col="red", ylim=c(0,1.5), ylab="Train MSE (Black) - Test MSE (Red)")
par(new=TRUE)
par(new=TRUE)
plot(deltas, train_mse,xlab="Shrinkage Parameter", ylim=c(0,1.5), ylab="")
library(gbm)
set.seed(1)
boost.hitters = gbm(Salary~.,data=trainset, distribution = "gaussian", n.trees=1000, interaction.depth = 4)
summary(boost.hitters)
boost.hitters$train.error
par(mfrow=c(1,2))
plot(boost.hitters, i="CAtBat")
plot(boost.hitters, i="CWalks")
rm(list=ls())
library(ISLR)
attach(Hitters)
completedata = na.omit(Hitters, col="Salary")
completedata["Salary"] = log(completedata["Salary"])
trainset = completedata[1:200,]
testset = completedata[201:nrow(completedata),]
library(gbm)
set.seed(1)
boost.hitters = gbm(Salary~.,data=trainset, distribution = "gaussian", n.trees=1000, interaction.depth = 4)
summary(boost.hitters)
boost.hitters$train.error
par(mfrow=c(1,2))
plot(boost.hitters, i="CAtBat")
plot(boost.hitters, i="CWalks")
test_mse = array(dim = 100)
train_mse = array(dim = 100)
deltas = array(dim = 100)
i = 1
for(shrkg in seq(0, 1, 0.01)) {
boost.hitters = gbm(Salary~.,
data=trainset,
distribution = "gaussian",
n.trees=1000,
interaction.depth = 4,
shrinkage=shrkg)
salary.boost=predict(boost.hitters, newdata=testset, n.trees=1000)
boost.hitters$train.error
# Print MSE and delta
MSE = mean((salary.boost - testset$Salary)^2)
T_MSE = mean(boost.hitters$train.error)
print(paste(shrkg, "   ", MSE))
test_mse[i] <- MSE
train_mse[i] <- T_MSE
deltas[i] <- shrkg
i = i+1
}
best_shrinkage = deltas[match(min(test_mse), test_mse)]
boosting_MSE = min(test_mse)
plot(deltas, test_mse,xlab="Shrinkage Parameter", col="red", ylim=c(0,1.5), ylab="Train MSE (Black) - Test MSE (Red)")
par(new=TRUE)
plot(deltas, train_mse,xlab="Shrinkage Parameter", ylim=c(0,1.5), ylab="")
rm(list=ls())
library(ISLR)
attach(Hitters)
completedata = na.omit(Hitters, col="Salary")
completedata["Salary"] = log(completedata["Salary"])
trainset = completedata[1:200,]
testset = completedata[201:nrow(completedata),]
library(gbm)
set.seed(1)
boost.hitters = gbm(Salary~.,data=trainset, distribution = "gaussian", n.trees=1000, interaction.depth = 4)
summary(boost.hitters)
boost.hitters$train.error
par(mfrow=c(1,2))
plot(boost.hitters, i="CAtBat")
plot(boost.hitters, i="CWalks")
boost.hitters$train.error
par(mfrow=c(1,2))
plot(boost.hitters, i="CAtBat")
plot(boost.hitters, i="CWalks")
test_mse = array(dim = 100)
train_mse = array(dim = 100)
deltas = array(dim = 100)
i = 1
for(shrkg in seq(0, 1, 0.01)) {
boost.hitters = gbm(Salary~.,
data=trainset,
distribution = "gaussian",
n.trees=1000,
interaction.depth = 4,
shrinkage=shrkg)
salary.boost=predict(boost.hitters, newdata=testset, n.trees=1000)
boost.hitters$train.error
# Print MSE and delta
MSE = mean((salary.boost - testset$Salary)^2)
T_MSE = mean(boost.hitters$train.error)
print(paste(shrkg, "   ", MSE))
test_mse[i] <- MSE
train_mse[i] <- T_MSE
deltas[i] <- shrkg
i = i+1
}
best_shrinkage = deltas[match(min(test_mse), test_mse)]
boosting_MSE = min(test_mse)
plot(deltas, test_mse,xlab="Shrinkage Parameter", col="red", ylim=c(0,1.5), ylab="Train MSE (Black) - Test MSE (Red)")
par(new=TRUE)
par(new=TRUE)
plot(deltas, train_mse,xlab="Shrinkage Parameter", ylim=c(0,1.5), ylab="")
library(ggplot2)
ggplot(Data, aes(x,y)) + geom_point() + geom_smooth()
lm.fit = lm(Salary~., data=trainset)
summary(lm.fit)
lm.pred = predict(lm.fit,testset)
least_square_MSE = mean((testset$Salary - lm.pred)^2)
x = model.matrix(Salary~.,completedata)[,-1]
y = completedata$Salary
train=sample(1:nrow(x), nrow(x)/2)
test=(-train )
y.test = y[test]
library(glmnet)
grid=10^seq(10,-2,length=100)
ridge.mod=glmnet(x[train,],y[train], alpha=0, lambda=grid)
ridge.pred = predict(ridge.mod, s=4, newx = x[test,])
ridge_MSE = mean(((ridge.pred - y.test)^2))
print(boosting_MSE)
print(least_square_MSE)
print(ridge_MSE)
summary(boost.hitters)
cv.out=cv.glmnet(x[train,],y[train],alpha=1)
plot(cv.out)
bestlam=cv.out$lambda.min
cv.out$lambda
sort(abs(lm.fit$coefficients))
lm.fit$coefficients
summary(lm.fit)
summary(lm.fit)$coefficients
summary(ridge.mod)
summary(ridge.mod)$a0
ridge.mod$a0
ridge.mod$df
ridge.mod$dim
ridge.mod$lambda
ridge.mod$nobs
ridge.mod$dev.ratio
ridge.mod$nulldev
ridge.mod$call
ridge.mod$jerr
library(randomForest)
bag.hitters = randomForest(Salary~.,data=trainset)
salary.bag = predict(bag.hitters, newdata=trainset)
plot(salary.bag, hitters.test)
plot(salary.bag, testset$Salary)
salary.bag
testset$Salary
dim(salary.bag)
salary.bag$dim
summary(salary.bag)
salary.bag
salary.bag[1]
salary.bag[1,1]
bag.hitters = randomForest(Salary~.,data=trainset, importance=TRUE)
salary.bag = predict(bag.hitters, newdata=trainset)
plot(unname(salary.bag), testset$Salary)
unname(salary.bag)
testset$Salary
bag.hitters = randomForest(Salary~.,data=trainset, importance=TRUE)
salary.bag = predict(bag.hitters, newdata=testset)
plot(unname(salary.bag), testset$Salary)
importance(bag.hitters)
bag.hitters$mse
mean(bag.hitters$mse)
mean(((unname(salary.bag))-testset$Salary)^2)
importance(bag.hitters)
bag.hitters$mse
x <- [482,133,46,24,6,5,2,1,0,1,0]
x <- list(482,133,46,24,6,5,2,1,0,1,0)
summary(x)
x <- c(482,133,46,24,6,5,2,1,0,1,0)
summary(x)
x <- c(0,1,2,3,4,5,6,7,8,9,10)
n <- c(482,133,46,24,6,5,2,1,0,1,0)
l <- list(x,n)
summary(l)
x <- c(0,1,2,3,4,5,6,7,8,9,10)
n <- c(482,133,46,24,6,5,2,1,0,1,0)
l <- table(x,n)
summary(l)
describe(l)
typeof(l)
class(l)
library(lattice)
histogram(a)
histogram(l)
histogram(n)
x <- 0:10
n <- c(482,133,46,24,6,5,2,1,0,1,0)
cbind(x,n)
typeof(l)
class(l)
library(lattice)
histogram(n)
histogram(x)
x
n
t = cbind(x,n)
typeof(t)
class(t)
library(lattice)
histogram(x)
histogram(t)
histogram(n)
?histogram
histogram(x, n)
densityplot(x, n)
histogram(x, data=n)
densityplot(x, data=n)
dim(t)
t
t = cbind(n,x)
typeof(t)
class(t)
dim(t)
summary(t)
summary(n)
frequency(n)
frequency(t)
t = cbind(x, n)
typeof(t)
dim(t)
frequency(t)
class(n)
n <- table(c(482,133,46,24,6,5,2,1,0,1,0))
t = cbind(x, n)
n
n <- c(482,133,46,24,6,5,2,1,0,1,0)
n
plot(t)
x <- 0:10
n <- c(482,133,46,24,6,5,2,1,0,1,0)
plot(x, data=n)
plot(x, n)
plot(x, n, type=h)
plot(x, n, type='h')
plot(x, n, type='h', lwd = 4)
plot(x, n, type='h', lwd = 40)
summary(n)
frequency(n, x)
hist(x, n)
hist(x, n, ylim = 500, xlim = 11)
hist(x, n, ylim = NULL, xlim = range(11))
hist(x, n, ylab = " Nombre de truc par machin", xlim = range(11))
hist(x, n, ylab = " Nombre de truc par machin", xlim = range(11), ylim = range(n))
hist(x, breaks = "Sturges", n, ylab = " Nombre de truc par machin", xlim = range(11), ylim = range(n))
hist(n, breaks = "Sturges", ylab = " Nombre de truc par machin", xlim = range(11), ylim = range(n))
range(11)
hist(n, breaks = "Sturges", ylab = " Nombre de truc par machin", xlim = range(0:11), ylim = range(n))
range(0:11)
hist(n)
dim(t)
hist(n, breaks = "Sturges", ylab = " Nombre de truc par machin", xlim = range(0:11), ylim = range(n))
n <- list(c(482,133,46,24,6,5,2,1,0,1,0))
t = cbind(x, n)
hist(n, breaks = "Sturges", ylab = " Nombre de truc par machin", xlim = range(0:11), ylim = range(n))
hist(t, breaks = "Sturges", ylab = " Nombre de truc par machin", xlim = range(0:11), ylim = range(n))
x <- c(0:10)
n <- c(482,133,46,24,6,5,2,1,0,1,0)
t = cbind(x, n)
hist(t, breaks = "Sturges", ylab = " Nombre de truc par machin", xlim = range(0:11), ylim = range(n))
t
?hist
?hist
x <- c(482,133,46,24,6,5,2,1,0,1,0)
h <- hist(x, breaks = 8,
main="Fréquence des galles par feuilles",
ylim=c(0,500),
xlab="Nombre de galles",
col="darkmagenta",
freq=TRUE
)
h$breaks
h$counts
h$counts
h$density
h <- hist(x, breaks = 8,
main="Fréquence des galles par feuilles",
ylim=c(0,500),
xlab="Nombre de galles",
col="darkmagenta"
)
h$breaks
hist(x)
h
h <- hist(x, breaks = 8, freq = TRUE,
main="Fréquence des galles par feuilles",
ylim=c(0,500),
xlab="Nombre de galles",
col="darkmagenta"
)
x <- c(482*0,133*1,46*2,24*3,6,5,2,1,0,1,0)
x
x <- c(490, 421, 483, 501, 497, 493, 468, 477, 481, 512, 481, 520, 489, 511, 534, 525, 466, 537, 519, 489, 442, 483, 489, 497, 401, 492, 470, 477, 495, 512, 480, 556, 493, 589)
h <- hist(x, breaks = 8, freq = TRUE,
main="Fréquence des galles par feuilles",
ylim=c(0,500),
xlab="Nombre de galles",
col="darkmagenta"
)
h
hist(x, breaks = 8, freq = TRUE,
main="Fréquence des galles par feuilles",
ylim=c(0,500),
xlab="Nombre de galles",
col="darkmagenta"
)
hist(x, breaks = 8, freq = TRUE,
main="Fréquence des galles par feuilles",
xlab="Nombre de galles",
col="darkmagenta"
)
hist(x, breaks = 8, freq = TRUE,
main="Température",
xlab="Températures",
ylab="Fréquences",
col="darkmagenta"
)
?boxplot
boxplot(maus_1)
maus_1 = c(8, 8, 3, 4, 3, 3, 3, 8)
maus_2 = c(4, 5, 4, 6, 6, 5)
boxplot(maus_1)
boxplot(maus_1, maus_2, names = c("Maus 1", "Maus 2"))
install.packages(c("ggplot2", "pls"))
install.packages(c("gbm", "glmnet", "ISLR", "randomForest"))
install.packages("FNN")
rm(list=ls())
workingDirectory = 'C:/Users/thsch/Desktop/ESL_Assignements_WS20172018/Assignement 1/R_Code'
setwd(workingDirectory)
load('ozone.RData')
ozone
ls(ozone)
str(ozone)
range(ozone)
colnames(ozone)
length(testset)
length(trainset)
dim(ozone)
# Mean of the columns
apply(ozone, 2, mean)
?apply
ozone[2]
plot(x = ozone[,1],
y = ozone[,2],
main = "Ozone vs Radiation",
xlab = "Ozone",
ylab = "Radiation",
xlim = c(0,170),
ylim = c(0,335)
)
# Give the chart file a name. If you don't want to save the file, ignore the png() and the dev.off().
png(file = "scatterplot_matrices.png")
pairs(ozone,
main = "Scatterplot Matrix")
# Save the file.
dev.off()
# For a direct display in R
pairs(ozone,
main = "Scatterplot Matrix")
# Pearson Correlation
cor(ozone)
# RSS function univariate
RSS <- function(y_true,y_predicted){
#print(dim(y_true))
res <- sum((y_true - y_predicted)^2)
return(res)
}
# Now fit a linear model using lm()
t_data <- ozone[trainset,]
lm_fit <- lm(t_data$ozone ~ t_data$radiation + t_data$temp + t_data$wind)
